{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3775f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tifffile\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "PROJECT_ROOT = Path(\"/mnt/home/dchhantyal/3d-cnn-classification\")\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "from model.model import ConvRNN, Config, resize_volume, NucleusDataset, DataLoader, RandomAugmentation3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40786b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR = (\n",
    "    \"/mnt/home/dchhantyal/3d-cnn-classification/data/nuclei_state_dataset\"  #\n",
    ")\n",
    "\n",
    "config = Config() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3309fefd-6273-4fad-a794-87c93f232765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize Model, Loss, and Optimizer\n",
    "model = ConvRNN(num_classes=config.num_classes).to(config.device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f404f022-8c0e-4d6d-bfac-d1520d586e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare DataLoaders\n",
    "full_dataset = NucleusDataset(root_dir=DATA_ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939472dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 605 total samples.\n",
      "Training on 484 samples, validating on 121 samples.\n",
      "Model has 97,731 trainable parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/dchhantyal/venvs/jupyter-gpu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    " # 1. Create a dataset for training WITH the augmentation transform\n",
    "train_full_dataset = NucleusDataset(\n",
    "    root_dir=DATA_ROOT_DIR, transform=RandomAugmentation3D()\n",
    ")\n",
    "\n",
    "# 2. Create a second dataset for validation WITHOUT the transform\n",
    "val_full_dataset = NucleusDataset(root_dir=DATA_ROOT_DIR, transform=None)\n",
    "\n",
    "# 3. Perform the stratified split on indices\n",
    "labels = [sample[1] for sample in train_full_dataset.samples]\n",
    "indices = list(range(len(train_full_dataset)))\n",
    "train_indices, val_indices = train_test_split(\n",
    "    indices, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# 4. Create Subsets using the correct dataset instance for each\n",
    "train_dataset = Subset(train_full_dataset, train_indices)\n",
    "val_dataset = Subset(val_full_dataset, val_indices)\n",
    "# --- END OF MODIFICATION ---\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Found {len(full_dataset)} total samples.\")\n",
    "print(\n",
    "    f\"Training on {len(train_dataset)} samples, validating on {len(val_dataset)} samples.\"\n",
    ")\n",
    "print(\n",
    "    f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbe4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] | Train Loss: 1.1040, Train Acc: 32.02% | Val Loss: 1.1059, Val Acc: 32.23% | Duration: 169.21s\n",
      "Epoch [2/300] | Train Loss: 1.0997, Train Acc: 32.02% | Val Loss: 1.0978, Val Acc: 33.88% | Duration: 168.37s\n",
      "Epoch [3/300] | Train Loss: 1.0963, Train Acc: 32.64% | Val Loss: 1.0924, Val Acc: 33.06% | Duration: 165.94s\n",
      "Epoch [4/300] | Train Loss: 1.0935, Train Acc: 34.30% | Val Loss: 1.0862, Val Acc: 33.88% | Duration: 166.00s\n",
      "Epoch [5/300] | Train Loss: 1.0935, Train Acc: 35.33% | Val Loss: 1.0840, Val Acc: 36.36% | Duration: 165.76s\n",
      "Epoch [6/300] | Train Loss: 1.0888, Train Acc: 37.60% | Val Loss: 1.0766, Val Acc: 39.67% | Duration: 165.58s\n",
      "Epoch [7/300] | Train Loss: 1.0865, Train Acc: 40.08% | Val Loss: 1.0741, Val Acc: 40.50% | Duration: 166.19s\n",
      "Epoch [8/300] | Train Loss: 1.0881, Train Acc: 38.84% | Val Loss: 1.0734, Val Acc: 41.32% | Duration: 166.10s\n",
      "Epoch [9/300] | Train Loss: 1.0871, Train Acc: 38.22% | Val Loss: 1.0672, Val Acc: 43.80% | Duration: 167.50s\n",
      "Epoch [10/300] | Train Loss: 1.0809, Train Acc: 38.22% | Val Loss: 1.0657, Val Acc: 41.32% | Duration: 166.14s\n",
      "Epoch [11/300] | Train Loss: 1.0805, Train Acc: 41.32% | Val Loss: 1.0582, Val Acc: 43.80% | Duration: 165.53s\n",
      "Epoch [12/300] | Train Loss: 1.0838, Train Acc: 38.84% | Val Loss: 1.0608, Val Acc: 40.50% | Duration: 166.13s\n",
      "Epoch [13/300] | Train Loss: 1.0798, Train Acc: 41.53% | Val Loss: 1.0546, Val Acc: 46.28% | Duration: 165.62s\n",
      "Epoch [14/300] | Train Loss: 1.0828, Train Acc: 39.67% | Val Loss: 1.0558, Val Acc: 44.63% | Duration: 163.90s\n",
      "Epoch [15/300] | Train Loss: 1.0766, Train Acc: 42.15% | Val Loss: 1.0546, Val Acc: 41.32% | Duration: 165.77s\n",
      "Epoch [16/300] | Train Loss: 1.0754, Train Acc: 44.01% | Val Loss: 1.0501, Val Acc: 45.45% | Duration: 164.40s\n"
     ]
    }
   ],
   "source": [
    "# 3. Training\n",
    "for epoch in range(config.num_epochs):  \n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(config.device), labels.to(\n",
    "            config.device\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # 4. Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(config.device), labels.to(\n",
    "                config.device\n",
    "            )  # Fixed: Config.DEVICE -> config.device\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{config.num_epochs}] | \" \n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}% | \"\n",
    "        f\"Duration: {epoch_duration:.2f}s\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2949c-7321-4dba-a849-b8b88139fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Final model save\n",
    "final_model_path = \"raw_masked_final_model.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")\n",
    "\n",
    "print(\"\\nTraining finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90629da-c790-42fb-8136-756736dcee16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-gpu",
   "language": "python",
   "name": "jupyter-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
