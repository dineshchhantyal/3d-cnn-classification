The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) modules/2.3-20240529   2) openblas/single-0.3.26
üöÄ Starting Run. All outputs will be saved to: training_outputs/20250718-010604
‚öôÔ∏è Using device: cuda
Hyperparameters: {'input_depth': 32, 'input_height': 32, 'input_width': 32, 'num_classes': 3, 'classes_names': ['mitotic', 'new_daughter', 'stable'], 'learning_rate': 0.0001, 'batch_size': 16, 'num_epochs': 300, 'num_input_channels': 3, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'early_stopping_patience': 25, 'max_samples_per_class': {'mitotic': 329, 'new_daughter': 329, 'stable': 329}, 'class_weights': [1.0, 1.0, 1.0]}

Loading datasets...
Dataset loaded with 987 total samples:
  - mitotic: 329 samples
  - new_daughter: 329 samples
  - stable: 329 samples
Dataset loaded with 987 total samples:
  - mitotic: 329 samples
  - new_daughter: 329 samples
  - stable: 329 samples
/mnt/home/dchhantyal/venvs/jupyter-gpu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Training on 789 samples, validating on 198 samples.
Model has 236,035 trainable parameters.

--- Starting Training ---
Traceback (most recent call last):
  File "/mnt/home/dchhantyal/3d-cnn-classification/model/rnn/cnn_model.py", line 357, in <module>
    main()
  File "/mnt/home/dchhantyal/3d-cnn-classification/model/rnn/cnn_model.py", line 303, in main
    train_loss, train_acc = train_one_epoch(
  File "/mnt/home/dchhantyal/3d-cnn-classification/model/rnn/cnn_model.py", line 147, in train_one_epoch
    outputs = model(inputs)  # The dataset now provides the correct shape
  File "/mnt/home/dchhantyal/venvs/jupyter-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/home/dchhantyal/venvs/jupyter-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/home/dchhantyal/3d-cnn-classification/model/rnn/model_utils.py", line 129, in forward
    batch_size, _, _, _, height, width = x.size()
ValueError: too many values to unpack (expected 6)
E0718 01:06:52.152000 2996500 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 2996529) of binary: /mnt/home/dchhantyal/venvs/jupyter-gpu/bin/python
Traceback (most recent call last):
  File "/mnt/home/dchhantyal/venvs/jupyter-gpu/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/home/dchhantyal/venvs/jupyter-gpu/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/mnt/home/dchhantyal/venvs/jupyter-gpu/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/mnt/home/dchhantyal/venvs/jupyter-gpu/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/mnt/home/dchhantyal/venvs/jupyter-gpu/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/home/dchhantyal/venvs/jupyter-gpu/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
cnn_model.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-18_01:06:52
  host      : workergpu089.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2996529)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: workergpu089: task 0: Exited with exit code 1
