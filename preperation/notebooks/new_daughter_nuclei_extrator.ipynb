{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ee2dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4cc9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "datasets = {\n",
    "    \"230212_stack6\": \"/mnt/ceph/users/lbrown/MouseData/Rebecca/230212_stack6/\",\n",
    "    \"220321_stack11\": \"/mnt/ceph/users/lbrown/MouseData/Eszter1/\",\n",
    "    \"221016_FUCCI_Nanog_stack_3\": \"/mnt/ceph/users/lbrown/Labels3DMouse/Abhishek/RebeccaData/221016_FUCCI_Nanog_stack_3/\",\n",
    "}\n",
    "\n",
    "# DATASET_SELECTION = \"230212_stack6\"  # Change this to switch datasets\n",
    "DATASET_SELECTION = \"221016_FUCCI_Nanog_stack_3\"  # Change this to switch datasets\n",
    "\n",
    "\n",
    "extractor_config = {\n",
    "    'time_window': 1,\n",
    "    'frame_offsets': [-1, 0, 1],\n",
    "    'crop_padding': 2.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c8c442",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     dataset_path \u001b[38;5;241m=\u001b[39m datasets[DATASET_SELECTION]\n\u001b[0;32m--> 110\u001b[0m     nucleus_states, path \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_SELECTION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     all_results[DATASET_SELECTION] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m: nucleus_states, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m: path}\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Create classification data and targets\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[0;34m(dataset_name, dataset_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(lineage_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m---> 13\u001b[0m nodes_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m edges_data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEdges\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create node properties\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Nodes'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Process selected dataset(s)\n",
    "def process_dataset(dataset_name, dataset_path):\n",
    "    \"\"\"Process a single dataset and return nucleus states\"\"\"\n",
    "    lineage_file = f\"{dataset_path}/LineageGraph.json\"\n",
    "    \n",
    "    with open(lineage_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    nodes_data = data[\"Nodes\"]\n",
    "    edges_data = data[\"Edges\"]\n",
    "\n",
    "    # Create node properties\n",
    "    nodes = {\n",
    "        node[\"Name\"]: {\"frame\": int(node[\"Name\"].split(\"_\")[0])} \n",
    "        for node in nodes_data\n",
    "    }\n",
    "\n",
    "    # Build parent-child relationships\n",
    "    parent_of = {edge[\"EndNodes\"][1]: edge[\"EndNodes\"][0] for edge in edges_data}\n",
    "    children_of = {}\n",
    "    for edge in edges_data:\n",
    "        parent_name = edge[\"EndNodes\"][0]\n",
    "        child_name = edge[\"EndNodes\"][1]\n",
    "        if parent_name not in children_of:\n",
    "            children_of[parent_name] = []\n",
    "        children_of[parent_name].append(child_name)\n",
    "    \n",
    "    # Identify nucleus states\n",
    "    def identify_nucleus_states(nodes, parent_of, children_of):\n",
    "        \"\"\"Identify new daughters, mitotic, death, and stable nuclei\"\"\"\n",
    "        nucleus_states = {\n",
    "            'new_daughters': [],\n",
    "            'mitotic': [],\n",
    "            'death': [],\n",
    "            'stable': []\n",
    "        }\n",
    "        \n",
    "        all_frames = [nodes[name][\"frame\"] for name in nodes]\n",
    "        last_frame = max(all_frames)\n",
    "        \n",
    "        for nucleus_name in sorted(nodes.keys()):\n",
    "            frame = nodes[nucleus_name][\"frame\"]\n",
    "            nucleus_id = int(nucleus_name.split('_')[1])\n",
    "            children = children_of.get(nucleus_name, [])\n",
    "            is_parent = nucleus_name in parent_of\n",
    "            \n",
    "            # Check if this is a new daughter (child of mitotic event)\n",
    "            if is_parent:\n",
    "                parent_name = parent_of[nucleus_name]\n",
    "                parent_children = children_of.get(parent_name, [])\n",
    "                if len(parent_children) >= 2:  # Parent had mitotic division\n",
    "                    nucleus_states['new_daughters'].append((frame, nucleus_id))\n",
    "                    continue\n",
    "            \n",
    "            # Check if undergoing mitosis (has 2+ children)\n",
    "            if len(children) >= 2:\n",
    "                nucleus_states['mitotic'].append((frame, nucleus_id))\n",
    "                continue\n",
    "            \n",
    "            # Check if dies (no children and not at last frame)\n",
    "            if len(children) == 0 and frame < last_frame:\n",
    "                nucleus_states['death'].append((frame, nucleus_id))\n",
    "                continue\n",
    "            \n",
    "            # Otherwise stable\n",
    "            nucleus_states['stable'].append((frame, nucleus_id))\n",
    "        \n",
    "        return nucleus_states\n",
    "\n",
    "    nucleus_states = identify_nucleus_states(nodes, parent_of, children_of)\n",
    "    return nucleus_states, dataset_path\n",
    "\n",
    "# Create classification data for all processed datasets\n",
    "def create_classification_data(nucleus_states):\n",
    "    \"\"\"Create classification data from nucleus states\"\"\"\n",
    "    classification_data = []\n",
    "    \n",
    "    for state_name, state_list in nucleus_states.items():\n",
    "        for frame, nucleus_id in state_list:\n",
    "            entry = {\n",
    "                'frame': frame,\n",
    "                'nucleus_id': nucleus_id,\n",
    "                'new_daughter': 1 if state_name == 'new_daughters' else 0,\n",
    "                'mitotic': 1 if state_name == 'mitotic' else 0,\n",
    "                'death': 1 if state_name == 'death' else 0,\n",
    "                'stable': 1 if state_name == 'stable' else 0\n",
    "            }\n",
    "            classification_data.append(entry)\n",
    "    \n",
    "    # Sort by frame and nucleus_id\n",
    "    classification_data.sort(key=lambda x: (x['frame'], x['nucleus_id']))\n",
    "    return classification_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a5af8-04e4-4bfe-bad1-e1b8f15d3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process dataset(s)\n",
    "all_results = {}\n",
    "\n",
    "if DATASET_SELECTION == \"all\":\n",
    "    for dataset_name, dataset_path in datasets.items():\n",
    "        try:\n",
    "            nucleus_states, path = process_dataset(dataset_name, dataset_path)\n",
    "            all_results[dataset_name] = {'states': nucleus_states, 'path': path}\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset_name}: {e}\")\n",
    "else:\n",
    "    dataset_path = datasets[DATASET_SELECTION]\n",
    "    nucleus_states, path = process_dataset(DATASET_SELECTION, dataset_path)\n",
    "    all_results[DATASET_SELECTION] = {'states': nucleus_states, 'path': path}\n",
    "\n",
    "# Create classification data and targets\n",
    "all_classification_data = {}\n",
    "all_new_daughter_targets = {}\n",
    "all_stable_targets = {}\n",
    "\n",
    "for dataset_name, dataset_info in all_results.items():\n",
    "    nucleus_states = dataset_info['states']\n",
    "\n",
    "    # Create classification data\n",
    "    classification_data = create_classification_data(nucleus_states)\n",
    "    all_classification_data[dataset_name] = classification_data\n",
    "\n",
    "    # Extract targets for extraction\n",
    "    all_new_daughter_targets[dataset_name] = nucleus_states['new_daughters']\n",
    "\n",
    "    stable_entries = [entry for entry in classification_data if entry['stable'] == 1]\n",
    "    stable_entries = random.sample(stable_entries, min(200, len(stable_entries)))  # Limit to 100 stable nuclei\n",
    "    all_stable_targets[dataset_name] = [\n",
    "        (entry[\"frame\"], entry[\"nucleus_id\"]) for entry in stable_entries\n",
    "    ]\n",
    "\n",
    "# Save classification data to data/labels directory\n",
    "output_dir = Path(\"../../data/labels\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save data for each dataset\n",
    "for dataset_name, classification_data in all_classification_data.items():\n",
    "    # Save full classification data\n",
    "    full_output_file = output_dir / f\"{dataset_name}_classification_full.txt\"\n",
    "    with open(full_output_file, 'w') as f:\n",
    "        f.write(\"frame\\tnucleus_id\\tnew_daughter\\tmitotic\\tdeath\\tstable\\n\")\n",
    "        for entry in classification_data:\n",
    "            f.write(f\"{entry['frame']}\\t{entry['nucleus_id']}\\t{entry['new_daughter']}\\t\"\n",
    "                    f\"{entry['mitotic']}\\t{entry['death']}\\t{entry['stable']}\\n\")\n",
    "    \n",
    "    # Save extraction plans\n",
    "    extraction_plan = {\n",
    "        'dataset': dataset_name,\n",
    "        'dataset_path': all_results[dataset_name]['path'],\n",
    "        'extraction_targets': {\n",
    "            'new_daughters': all_new_daughter_targets[dataset_name],\n",
    "            'stable': all_stable_targets[dataset_name]\n",
    "        },\n",
    "        'extractor_config': extractor_config\n",
    "    }\n",
    "    \n",
    "    plan_file = output_dir / f\"{dataset_name}_nucleus_extraction_plan.json\"\n",
    "    with open(plan_file, 'w') as f:\n",
    "        json.dump(extraction_plan, f, indent=2)\n",
    "\n",
    "print(f\"✅ Classification data saved for {len(all_results)} dataset(s)\")\n",
    "new_daughter_count = sum(len(targets) for targets in all_new_daughter_targets.values())\n",
    "stable_count = sum(len(targets) for targets in all_stable_targets.values())\n",
    "print(f\"🎯 Targets: {new_daughter_count} new daughters, {stable_count} stable nuclei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a8bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract nucleus images\n",
    "\n",
    "python_dir = os.path.join(os.path.dirname(os.getcwd()), 'python')\n",
    "if python_dir not in sys.path:\n",
    "    sys.path.append(python_dir)\n",
    "\n",
    "from nucleus_extractor_manager import NucleusExtractorManager, NucleusExtractorConfig\n",
    "manager_available = True\n",
    "\n",
    "\n",
    "extraction_results = {}\n",
    "\n",
    "for dataset_name in all_results.keys():\n",
    "    new_daughter_targets = all_new_daughter_targets[dataset_name]\n",
    "    stable_targets = all_stable_targets[dataset_name]\n",
    "    dataset_path = all_results[dataset_name]['path']\n",
    "\n",
    "    print(f\"Processing {dataset_name}: {len(new_daughter_targets)} new daughters, {len(stable_targets)} stable\")\n",
    "\n",
    "    try:\n",
    "        # Extract new daughter nuclei\n",
    "        if new_daughter_targets:\n",
    "            new_daughter_config = NucleusExtractorConfig()\n",
    "            manager = NucleusExtractorManager(dataset_path, new_daughter_config)\n",
    "\n",
    "            for i, (frame, nucleus_id) in enumerate(new_daughter_targets):\n",
    "                try:\n",
    "                    result = manager.extract_nucleus_time_series(nucleus_id, frame)\n",
    "\n",
    "                    if result and result[\"extraction_success\"]:\n",
    "                        result[\"event_type\"] = \"new_daughter\"\n",
    "                        result[\"is_mitotic\"] = 0\n",
    "                        result[\"is_death\"] = 0\n",
    "                        result[\"new_daughter\"] = 1\n",
    "                        result[\"stable\"] = 0\n",
    "                        output_dir = manager.save_extraction_results([result], f\"{dataset_name}_new_daughter\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "        # Extract stable nuclei\n",
    "        if stable_targets:\n",
    "            stable_config = NucleusExtractorConfig()\n",
    "\n",
    "            manager = NucleusExtractorManager(dataset_path, stable_config)\n",
    "\n",
    "            for i, (frame, nucleus_id) in enumerate(stable_targets):\n",
    "                try:\n",
    "                    result = manager.extract_nucleus_time_series(nucleus_id, frame)\n",
    "\n",
    "                    if result and result[\"extraction_success\"]:\n",
    "                        result[\"event_type\"] = \"stable\"\n",
    "                        result[\"is_mitotic\"] = 0\n",
    "                        result[\"is_death\"] = 0\n",
    "                        result[\"new_daughter\"] = 0\n",
    "                        result[\"stable\"] = 1\n",
    "                        output_dir = manager.save_extraction_results(\n",
    "                            [result], f\"{dataset_name}_stable\"\n",
    "                        )\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting from {dataset_name}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86bc62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nuclei were extracted\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "if extraction_results:\n",
    "    total_new_daughters = sum(\n",
    "        results.get(\"new_daughter\", {}).get(\"count\", 0)\n",
    "        for results in extraction_results.values()\n",
    "    )\n",
    "    total_stable = sum(\n",
    "        results.get(\"stable\", {}).get(\"count\", 0)\n",
    "        for results in extraction_results.values()\n",
    "    )\n",
    "    print(\n",
    "        f\"✅ Extraction complete: {total_new_daughters} new daughters, {total_stable} stable nuclei\"\n",
    "    )\n",
    "\n",
    "    for dataset_name, results in extraction_results.items():\n",
    "        if \"new_daughter\" in results:\n",
    "            print(\n",
    "                f\"  {dataset_name} new daughters: {results['new_daughter']['count']} → {results['new_daughter']['output_dir']}\"\n",
    "            )\n",
    "        if \"stable\" in results:\n",
    "            print(\n",
    "                f\"  {dataset_name} stable: {results['stable']['count']} → {results['stable']['output_dir']}\"\n",
    "            )\n",
    "else:\n",
    "    print(\"No nuclei were extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c7b4d-6f56-4a47-9249-f980b1c7bbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-gpu",
   "language": "python",
   "name": "jupyter-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
